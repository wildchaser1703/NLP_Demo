{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0245a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0312f02",
   "metadata": {},
   "source": [
    "### Stemming vs Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd302fa",
   "metadata": {},
   "source": [
    "### Stemming:\n",
    "1. Convert to its base, remove suffix to get base words -> \"talking\": \"talk\", \"eating\": \"eat\", \"adjustable\": \"adjust\"\n",
    "2. No proper rules: \"ability: \"abil\"\n",
    "3. Spacy does not provide inbuilt support for Stemming\n",
    "### Lemmatization\n",
    "1. Use linguistic knowledge to derive a base word -> \"ate\": \"eat\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6a74d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af814517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cabcd1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "reads | read\n",
      "remarkable | remark\n",
      "causality | causal\n",
      "rendering | render\n",
      "ability | abil\n"
     ]
    }
   ],
   "source": [
    "## Stemming in NLTK\n",
    "words = [\"eating\", \"eats\", \"reads\", \"remarkable\", \"causality\", \"rendering\", \"ability\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"|\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80d07c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | eat\n",
      "adjustable | adjustable\n",
      "remarkable | remarkable\n",
      "rafting | raft\n",
      "meeting | meeting\n",
      "better | well\n",
      "gone | go\n"
     ]
    }
   ],
   "source": [
    "## Lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"eating eats eat ate adjustable remarkable rafting meeting better gone\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token , \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3550526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resting | rest | 10960894369163974213\n",
      "must | must | 7290638946010101875\n",
      "you | you | 7624161793554793053\n",
      "will | will | 18307573501153647118\n",
      ", | , | 2593208677638477497\n",
      "for | for | 16037325823156266367\n",
      "loss | loss | 80859674766354010\n",
      "shall | shall | 7091588059074233151\n",
      "you | you | 7624161793554793053\n",
      "suffer | suffer | 12627334617871297789\n",
      "- | - | 9153284864653046197\n",
      "Baby | Baby | 16275649885857980966\n",
      "Yoda | Yoda | 52316164129200410\n",
      "and | and | 2283656566040971221\n",
      "other | other | 1176656782636220709\n",
      "quotes | quote | 5727435701664101727\n",
      "he | he | 1655312771067108281\n",
      "does | do | 2158845516055552166\n",
      "n't | not | 447765159362469301\n",
      "know | know | 7743033266031195906\n",
      "about | about | 942632335873952620\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Resting must you will, for loss shall you suffer - Baby Yoda and other quotes he doesn't know about\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token , \"|\", token.lemma_ , \"|\", token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db9c5a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95392da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brotha | Brother\n",
      "that | that\n",
      "shit | shit\n",
      "slaps | slap\n",
      ", | ,\n",
      "bruh | Brother\n",
      "; | ;\n",
      "Skibidi | skibidi\n",
      "this | this\n",
      ", | ,\n",
      "skibidi | skibidi\n",
      "that | that\n",
      ", | ,\n",
      "YOLO | yolo\n",
      "! | !\n",
      "! | !\n"
     ]
    }
   ],
   "source": [
    "## How do you interpret slangs? -> attribute_ruler\n",
    "ar = nlp.get_pipe(\"attribute_ruler\")\n",
    "## Add exception for Brother, case-sensitive\n",
    "ar.add([[{\"TEXT\":\"Brotha\"}],[{\"TEXT\":\"bruh\"}]], {\"LEMMA\": \"Brother\"})\n",
    "\n",
    "doc = nlp(\"Brotha that shit slaps, bruh; Skibidi this, skibidi that, YOLO!!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8ba41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
